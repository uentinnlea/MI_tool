{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Draw\n",
    "import torch\n",
    "from torch.nn import Sequential, Linear, ReLU, CrossEntropyLoss\n",
    "from torch_geometric.data import Dataset, Data\n",
    "import torch.nn.functional as F \n",
    "import os.path as osp\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Check if MPS (Apple GPU support) is available and use it; otherwise, use CPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated memory usage: 0.00 GB\n",
      "Total memory: 16.00 GB\n",
      "Available memory: 3.15 GB\n",
      "The dataset should fit into the available memory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import psutil\n",
    "\n",
    "# Path to your Parquet file\n",
    "# file_path = '/Users/leo/Desktop/Dataset/polyOne_Dataset/raw/polyOne_aa.parquet'\n",
    "# file_path = '/Users/lpc_0066/Desktop/Dataset/polyOne_Dataset/raw/polyOne_aa.parquet'\n",
    "# file_path = \"/Users/lpc_0066/Desktop/Dataset/ZINC_250k/raw/250k_rndm_zinc_drugs_clean_3.csv\"\n",
    "file_path = \"/Users/lpc_0066/Desktop/Dataset/From paper/10.26434:chemrxiv.12746948/solubility_data/raw/solubility_data.csv\"\n",
    "# file_path = \"/Users/leo/Desktop/Dataset/aqueous_solubility/raw/curated-solubility-dataset.csv\"\n",
    "\n",
    "# Step 1: Estimate Dataset Size\n",
    "# Load the dataset\n",
    "# full_df = pd.read_parquet(file_path, engine='pyarrow')\n",
    "full_df = pd.read_csv(file_path)\n",
    "\n",
    "# Take a sample of the dataset to estimate memory usage per row\n",
    "sample_df = full_df.sample(n=500)\n",
    "\n",
    "# Estimate total memory usage\n",
    "estimated_memory_per_row = sample_df.memory_usage(deep=True).sum() / len(sample_df)\n",
    "total_rows = len(full_df)\n",
    "estimated_total_memory = estimated_memory_per_row * total_rows\n",
    "\n",
    "print(f\"Estimated memory usage: {estimated_total_memory / (1024**3):.2f} GB\")\n",
    "\n",
    "# Step 2: Check Available Memory\n",
    "# Get total and available memory\n",
    "memory_info = psutil.virtual_memory()\n",
    "total_memory = memory_info.total\n",
    "available_memory = memory_info.available\n",
    "\n",
    "print(f\"Total memory: {total_memory / (1024**3):.2f} GB\")\n",
    "print(f\"Available memory: {available_memory / (1024**3):.2f} GB\")\n",
    "\n",
    "# Step 3: Compare Dataset Size to Available Memory\n",
    "if estimated_total_memory < available_memory:\n",
    "    print(\"The dataset should fit into the available memory.\")\n",
    "else:\n",
    "    print(\"The dataset is too large to fit into the available memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "The column name of smiles in csv dataset should be 'smiles'\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Dataset\n",
    "from typing import Callable, Any, Optional\n",
    "\n",
    "# root should not include raw folder.\n",
    "# root = \"/Users/leo/Desktop/Dataset/polyOne_Dataset\"\n",
    "# root = \"/Users/lpc_0066/Desktop/Dataset/polyOne_Dataset/\"\n",
    "# root = \"/Users/lpc_0066/Desktop/Dataset/ZINC_250k/\"\n",
    "root = \"/Users/lpc_0066/Desktop/Dataset/From paper/10.26434:chemrxiv.12746948/solubility_data\"\n",
    "# root = \"/Users/leo/Desktop/Dataset/aqueous_solubility\"\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Only support molecule not polymer.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 root, \n",
    "                 filename, \n",
    "                 target_name, \n",
    "                 transform: Callable[..., Any] | None = None, \n",
    "                 pre_transform: Callable[..., Any] | None = None, \n",
    "                 pre_filter: Callable[..., Any] | None = None) -> None:\n",
    "        \n",
    "        self.filename = filename\n",
    "        self.target_name = target_name\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    " \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return self.filename\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self): # self.data should be defined here or error occurs cuz super() calls processed_file_names property.\n",
    "        # self.data = pd.read_parquet(self.raw_paths[0])\n",
    "        self.data = pd.read_csv(self.raw_paths[0])\n",
    "        return [f\"data_{i}.pt\" for i in range(len(self.data))]\n",
    "    \n",
    "    def download(self):\n",
    "        # Used when the file is downloaded from url\n",
    "        pass\n",
    "\n",
    "    def len(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def get(self, idx):\n",
    "        data = torch.load(osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "        return data.to(device)\n",
    "    \n",
    "    def process(self):\n",
    "        for index, row in self.data.iterrows():\n",
    "            mol = Chem.MolFromSmiles(row.smiles)\n",
    "            node_feats = self.get_node_features(mol)\n",
    "            edge_feats = self.get_edge_features(mol)\n",
    "            edge_index = self.get_adjacency_info(mol)\n",
    "            label = self.get_labels(row[self.target_name])\n",
    "            structure_id = [[row.smiles]]\n",
    "            data = Data(x=node_feats, \n",
    "                        edge_index=edge_index,\n",
    "                        edge_attr=edge_feats,\n",
    "                        y=label,\n",
    "                        structure_id=structure_id)\n",
    "            torch.save(data, osp.join(self.processed_dir, f'data_{index}.pt'))\n",
    "            \n",
    "    def get_node_features(self, mol):\n",
    "        all_node_features = []\n",
    "        for atom in mol.GetAtoms():\n",
    "            node_features = F.one_hot(torch.tensor(atom.GetAtomicNum()-1), num_classes=118) \n",
    "            node_features = node_features.tolist()\n",
    "\n",
    "            node_features.append(atom.GetDegree())\n",
    "            node_features.append(atom.GetFormalCharge())\n",
    "            node_features.append(atom.GetHybridization())\n",
    "            node_features.append(atom.GetIsAromatic())\n",
    "            node_features.append(atom.GetTotalNumHs())\n",
    "            node_features.append(atom.GetNumRadicalElectrons())\n",
    "            node_features.append(atom.IsInRing())\n",
    "            node_features.append(atom.GetChiralTag())\n",
    "            node_features.append(atom.GetMass())\n",
    "\n",
    "            all_node_features.append(node_features)\n",
    "        return torch.tensor(all_node_features, dtype=torch.float32)\n",
    "            \n",
    "    def get_edge_features(self, mol):\n",
    "        all_edge_features = []\n",
    "        for bond in mol.GetBonds():\n",
    "            edge_features = []\n",
    "            edge_features.append(torch.tensor(bond.GetBondTypeAsDouble()))\n",
    "            edge_features.append(bond.IsInRing())\n",
    "            edge_features.append(bond.GetStereo())\n",
    "            edge_features.append(bond.GetIsConjugated())\n",
    "\n",
    "            all_edge_features.append(edge_features)\n",
    "        return torch.tensor(all_edge_features, dtype=torch.float32)\n",
    "    \n",
    "    def get_adjacency_info(self, mol):\n",
    "        edge_indices = []\n",
    "        for bond in mol.GetBonds():\n",
    "            i = bond.GetBeginAtomIdx()\n",
    "            j = bond.GetEndAtomIdx()\n",
    "            edge_indices += [[i, j], [j,i]]\n",
    "\n",
    "        edge_indices_tensor = torch.tensor(edge_indices, dtype=torch.int64)\n",
    "        return edge_indices_tensor.T\n",
    "    \n",
    "    def get_labels(self, label):\n",
    "        tensor_label = torch.tensor([[label]])\n",
    "        return tensor_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = GraphDataset(root=root, filename=\"solubility_data.csv\", target_name=\"logS0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[10, 127], edge_index=[2, 20], edge_attr=[10, 4], y=[1, 1], structure_id=[1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[134]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "dataset = dataset.shuffle()\n",
    "#学習データとテストデータに分割する。\n",
    "from sklearn.model_selection import train_test_split\n",
    "dataset_train, dataset_test = train_test_split(dataset, test_size=0.2)\n",
    "\n",
    "#バッチサイズ\n",
    "batch_size = 64\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GCN model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 64, output_dim: int = 1):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, data) -> torch.Tensor:\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        edge_index = edge_index.to(torch.int64)\n",
    "        \n",
    "        # GCN layers with ReLU activation\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # Global mean pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        # Fully connected layers with dropout and ReLU\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.7935696623542092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Eval Loss: 1.208775798479716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 1.3519518808885054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Eval Loss: 1.232139269510905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 1.3088043494658037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Eval Loss: 1.2083762486775715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 1.2805947173725476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Eval Loss: 1.1920847495396931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 1.28975539857691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Eval Loss: 1.1782976786295574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 1.2612739205360413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Eval Loss: 1.1764728426933289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 1.2276048551906238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Eval Loss: 1.1703354914983113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 1.2479055252942173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Eval Loss: 1.1667586366335552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 1.2178717309778386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Eval Loss: 1.1697170734405518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 1.2258867567235774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Eval Loss: 1.155941108862559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Train Loss: 1.2131500081582502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Eval Loss: 1.1539507706960042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Train Loss: 1.2335076982324773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Eval Loss: 1.1490357120831807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▎   | 7/11 [00:52<00:30,  7.59s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "device_train = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device_eval = torch.device('cpu')\n",
    "\n",
    "input_dim = dataset.num_node_features\n",
    "gcn = GCN(input_dim)\n",
    "\n",
    "optimizer = optim.AdamW(gcn.parameters(), lr=0.001)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "# Lists to record losses\n",
    "epoch_loss_train = []\n",
    "epoch_loss_test = []\n",
    "\n",
    "# Define training loop\n",
    "def train(model, device, loader, optimizer, criterion):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "    for data in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        data = data.to(device)  # Move data to the training device\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Define evaluation loop\n",
    "def evaluate(model, device, loader, criterion):\n",
    "    model.to(device)  # Move model to evaluation device\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    with torch.inference_mode():\n",
    "        for data in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "            data = data.to(device)  # Move data to the evaluation device\n",
    "            output = model(data)\n",
    "            loss = criterion(output, data.y)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Training phase\n",
    "    gcn.to(device_train)  # Ensure model is on the training device\n",
    "    train_loss = train(gcn, device_train, loader_train, optimizer, criterion)\n",
    "    epoch_loss_train.append(train_loss)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss}\")\n",
    "\n",
    "    # Evaluation phase\n",
    "    gcn.to(device_eval)  # Ensure model is on the evaluat\n",
    "    eval_loss = evaluate(gcn, device_eval, loader_test, criterion)\n",
    "    epoch_loss_test.append(eval_loss)\n",
    "    print(f\"Epoch {epoch+1}, Eval Loss: {eval_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(epoch_loss_train)), epoch_loss_train, label=\"Train\")\n",
    "plt.plot(range(len(epoch_loss_test)), epoch_loss_test, label=\"Test\")\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn.eval()  # ネットワークを推論モードに切り替える\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for data in loader_test:\n",
    "        label = data.y\n",
    "        output = gcn(data)  # ネットワークにテストデータを入力して予測結果を取得\n",
    "        labels.append(label)\n",
    "        predictions.append(output)\n",
    "\n",
    "predictions = torch.cat(predictions, dim=0)  # 予測結果を結合して1つのテンソルにする\n",
    "labels = torch.cat(labels, dim=0)\n",
    "plt.scatter(labels.float(), predictions.float())\n",
    "plt.xlabel('Measured')\n",
    "plt.ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
