{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Draw\n",
    "import torch\n",
    "from torch.nn import Sequential, Linear, ReLU, CrossEntropyLoss\n",
    "from torch_geometric.data import Dataset, Data\n",
    "import torch.nn.functional as F \n",
    "import os.path as osp\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Check if MPS (Apple GPU support) is available and use it; otherwise, use CPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated memory usage: 0.01 GB\n",
      "Total memory: 16.00 GB\n",
      "Available memory: 5.84 GB\n",
      "The dataset should fit into the available memory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import psutil\n",
    "\n",
    "# Path to your Parquet file\n",
    "# file_path = '/Users/leo/Desktop/Dataset/polyOne_Dataset/raw/polyOne_aa.parquet'\n",
    "# file_path = '/Users/lpc_0066/Desktop/Dataset/polyOne_Dataset/raw/polyOne_aa.parquet'\n",
    "# file_path = \"/Users/lpc_0066/Desktop/Dataset/ZINC_250k/250k_rndm_zinc_drugs_clean_3.csv\"\n",
    "file_path = \"/Users/leo/Desktop/Dataset/aqueous_solubility/raw/curated-solubility-dataset.csv\"\n",
    "\n",
    "# Step 1: Estimate Dataset Size\n",
    "# Load the dataset\n",
    "# full_df = pd.read_parquet(file_path, engine='pyarrow')\n",
    "full_df = pd.read_csv(file_path)\n",
    "\n",
    "# Take a sample of the dataset to estimate memory usage per row\n",
    "sample_df = full_df.sample(n=1000)\n",
    "\n",
    "# Estimate total memory usage\n",
    "estimated_memory_per_row = sample_df.memory_usage(deep=True).sum() / len(sample_df)\n",
    "total_rows = len(full_df)\n",
    "estimated_total_memory = estimated_memory_per_row * total_rows\n",
    "\n",
    "print(f\"Estimated memory usage: {estimated_total_memory / (1024**3):.2f} GB\")\n",
    "\n",
    "# Step 2: Check Available Memory\n",
    "# Get total and available memory\n",
    "memory_info = psutil.virtual_memory()\n",
    "total_memory = memory_info.total\n",
    "available_memory = memory_info.available\n",
    "\n",
    "print(f\"Total memory: {total_memory / (1024**3):.2f} GB\")\n",
    "print(f\"Available memory: {available_memory / (1024**3):.2f} GB\")\n",
    "\n",
    "# Step 3: Compare Dataset Size to Available Memory\n",
    "if estimated_total_memory < available_memory:\n",
    "    print(\"The dataset should fit into the available memory.\")\n",
    "else:\n",
    "    print(\"The dataset is too large to fit into the available memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Dataset\n",
    "from typing import Callable, Any, Optional\n",
    "\n",
    "# root = \"/Users/leo/Desktop/Dataset/polyOne_Dataset\"\n",
    "# root = \"/Users/lpc_0066/Desktop/Dataset/polyOne_Dataset/\"\n",
    "# root = \"/Users/lpc_0066/Desktop/Dataset/ZINC_250k\"\n",
    "root = \"/Users/leo/Desktop/Dataset/aqueous_solubility\"\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Only support molecule not polymer.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 root, \n",
    "                 filename, \n",
    "                 target_name, \n",
    "                 transform: Callable[..., Any] | None = None, \n",
    "                 pre_transform: Callable[..., Any] | None = None, \n",
    "                 pre_filter: Callable[..., Any] | None = None) -> None:\n",
    "        \n",
    "        self.filename = filename\n",
    "        self.target_name = target_name\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    " \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return self.filename\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self): # self.data should be defined here or error occurs cuz super() calls processed_file_names property.\n",
    "        # self.data = pd.read_parquet(self.raw_paths[0])\n",
    "        self.data = pd.read_csv(self.raw_paths[0])\n",
    "        return [f\"data_{i}.pt\" for i in range(len(self.data))]\n",
    "    \n",
    "    def download(self):\n",
    "        # Used when the file is downloaded from url\n",
    "        pass\n",
    "\n",
    "    def len(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def get(self, idx):\n",
    "        data = torch.load(osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "        return data.to(device)\n",
    "    \n",
    "    def process(self):\n",
    "        for index, row in self.data.iterrows():\n",
    "            mol = Chem.MolFromSmiles(row.smiles)\n",
    "            node_feats = self.get_node_features(mol)\n",
    "            edge_feats = self.get_edge_features(mol)\n",
    "            edge_index = self.get_adjacency_info(mol)\n",
    "            label = self.get_labels(row[self.target_name])\n",
    "            structure_id = [[row.smiles]]\n",
    "            data = Data(x=node_feats, \n",
    "                        edge_index=edge_index,\n",
    "                        edge_attr=edge_feats,\n",
    "                        y=label,\n",
    "                        structure_id=structure_id)\n",
    "            torch.save(data, osp.join(self.processed_dir, f'data_{index}.pt'))\n",
    "            \n",
    "    def get_node_features(self, mol):\n",
    "        all_node_features = []\n",
    "        for atom in mol.GetAtoms():\n",
    "            node_features = F.one_hot(torch.tensor(atom.GetAtomicNum()-1), num_classes=118) \n",
    "            node_features = node_features.tolist()\n",
    "\n",
    "            node_features.append(atom.GetDegree())\n",
    "            node_features.append(atom.GetFormalCharge())\n",
    "            node_features.append(atom.GetHybridization())\n",
    "            node_features.append(atom.GetIsAromatic())\n",
    "            node_features.append(atom.GetTotalNumHs())\n",
    "            node_features.append(atom.GetNumRadicalElectrons())\n",
    "            node_features.append(atom.IsInRing())\n",
    "            node_features.append(atom.GetChiralTag())\n",
    "            node_features.append(atom.GetMass())\n",
    "\n",
    "            all_node_features.append(node_features)\n",
    "        return torch.tensor(all_node_features, dtype=torch.float32)\n",
    "            \n",
    "    def get_edge_features(self, mol):\n",
    "        all_edge_features = []\n",
    "        for bond in mol.GetBonds():\n",
    "            edge_features = []\n",
    "            edge_features.append(torch.tensor(bond.GetBondTypeAsDouble()))\n",
    "            edge_features.append(bond.IsInRing())\n",
    "            edge_features.append(bond.GetStereo())\n",
    "            edge_features.append(bond.GetIsConjugated())\n",
    "\n",
    "            all_edge_features.append(edge_features)\n",
    "        return torch.tensor(all_edge_features, dtype=torch.float32)\n",
    "    \n",
    "    def get_adjacency_info(self, mol):\n",
    "        edge_indices = []\n",
    "        for bond in mol.GetBonds():\n",
    "            i = bond.GetBeginAtomIdx()\n",
    "            j = bond.GetEndAtomIdx()\n",
    "            edge_indices += [[i, j], [j,i]]\n",
    "\n",
    "        edge_indices_tensor = torch.tensor(edge_indices, dtype=torch.int64)\n",
    "        return edge_indices_tensor.T\n",
    "    \n",
    "    def get_labels(self, label):\n",
    "        tensor_label = torch.tensor([[label]])\n",
    "        return tensor_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GraphDataset(root=root, filename=\"curated-solubility-dataset.csv\", target_name=\"Solubility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[23, 127], edge_index=[2, 50], edge_attr=[25, 4], y=[1, 1], structure_id=[1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "dataset = dataset.shuffle()\n",
    "#学習データとテストデータに分割する。\n",
    "from sklearn.model_selection import train_test_split\n",
    "dataset_train, dataset_test = train_test_split(dataset, test_size=0.2)\n",
    "\n",
    "#バッチサイズ\n",
    "batch_size = 64\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GCN model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 64, output_dim: int = 1):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, data) -> torch.Tensor:\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        edge_index = edge_index.to(torch.int64)\n",
    "        \n",
    "        # GCN layers with ReLU activation\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # Global mean pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        # Fully connected layers with dropout and ReLU\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.9313265705108642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Eval Loss: 1.8131747245788574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 1.7251832084655763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Eval Loss: 1.6634995862841606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 1.6311508340835572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Eval Loss: 1.5504405684769154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 1.5347203903198243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Eval Loss: 1.50291109085083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 1.4593891878128051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Eval Loss: 1.3959333561360836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 1.3619255032539368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Eval Loss: 1.3377492055296898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 1.3138109979629518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Eval Loss: 1.3465438298881054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# Training phase\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     gcn\u001b[38;5;241m.\u001b[39mto(device_train)  \u001b[38;5;66;03m# Ensure model is on the training device\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgcn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     epoch_loss_train\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     22\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Move data to the training device\u001b[39;00m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 24\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, data\u001b[38;5;241m.\u001b[39my)\n\u001b[1;32m     26\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/graph/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/graph/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 22\u001b[0m, in \u001b[0;36mGCN.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     19\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m edge_index\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint64)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# GCN layers with ReLU activation\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x, edge_index)\n",
      "File \u001b[0;32m~/anaconda3/envs/graph/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/graph/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/graph/lib/python3.11/site-packages/torch_geometric/nn/conv/gcn_conv.py:241\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    239\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43mgcn_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# yapf: disable\u001b[39;49;00m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimproved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_self_loops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached:\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index \u001b[38;5;241m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[0;32m~/anaconda3/envs/graph/lib/python3.11/site-packages/torch_geometric/nn/conv/gcn_conv.py:103\u001b[0m, in \u001b[0;36mgcn_norm\u001b[0;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m add_remaining_self_loops(\n\u001b[1;32m    100\u001b[0m         edge_index, edge_weight, fill_value, num_nodes)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edge_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 103\u001b[0m     edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m row, col \u001b[38;5;241m=\u001b[39m edge_index[\u001b[38;5;241m0\u001b[39m], edge_index[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    107\u001b[0m idx \u001b[38;5;241m=\u001b[39m col \u001b[38;5;28;01mif\u001b[39;00m flow \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_to_target\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m row\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "device_train = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device_eval = torch.device('cpu')\n",
    "\n",
    "input_dim = dataset.num_node_features\n",
    "gcn = GCN(input_dim)\n",
    "\n",
    "optimizer = optim.AdamW(gcn.parameters(), lr=0.001)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "# Lists to record losses\n",
    "epoch_loss_train = []\n",
    "epoch_loss_test = []\n",
    "\n",
    "# Define training loop\n",
    "def train(model, device, loader, optimizer, criterion):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "    for data in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        data = data.to(device)  # Move data to the training device\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Define evaluation loop\n",
    "def evaluate(model, device, loader, criterion):\n",
    "    model.to(device)  # Move model to evaluation device\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    with torch.inference_mode():\n",
    "        for data in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "            data = data.to(device)  # Move data to the evaluation device\n",
    "            output = model(data)\n",
    "            loss = criterion(output, data.y)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Training phase\n",
    "    gcn.to(device_train)  # Ensure model is on the training device\n",
    "    train_loss = train(gcn, device_train, loader_train, optimizer, criterion)\n",
    "    epoch_loss_train.append(train_loss)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss}\")\n",
    "\n",
    "    # Evaluation phase\n",
    "    gcn.to(device_eval)  # Ensure model is on the evaluat\n",
    "    eval_loss = evaluate(gcn, device_eval, loader_test, criterion)\n",
    "    epoch_loss_test.append(eval_loss)\n",
    "    print(f\"Epoch {epoch+1}, Eval Loss: {eval_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(epoch_loss_train)), epoch_loss_train, label=\"Train\")\n",
    "plt.plot(range(len(epoch_loss_test)), epoch_loss_test, label=\"Test\")\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn.eval()  # ネットワークを推論モードに切り替える\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for data in loader_test:\n",
    "        label = data.y\n",
    "        output = gcn(data)  # ネットワークにテストデータを入力して予測結果を取得\n",
    "        labels.append(label)\n",
    "        predictions.append(output)\n",
    "\n",
    "predictions = torch.cat(predictions, dim=0)  # 予測結果を結合して1つのテンソルにする\n",
    "labels = torch.cat(labels, dim=0)\n",
    "plt.scatter(labels.float(), predictions.float())\n",
    "plt.xlabel('Measured')\n",
    "plt.ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
